{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84fdf10d-d4ec-4ced-bd24-138e96344d2b",
   "metadata": {},
   "source": [
    "## Recommendation system project\n",
    "\n",
    "In current, work I will create recommender system of text posts. As raw data, I will use the following tables:\n",
    "\n",
    "**user_data:**\n",
    "\n",
    "| Field name  | Overview  |\n",
    "|---|---|\n",
    "| age  | User age (in profile)  |\n",
    "| city  | User city (in profile)  |\n",
    "| country  | User country (in profile)  |\n",
    "| exp_group  | Experimental group: some encrypted category  |\n",
    "| gender  | User Gender  |\n",
    "| id  | Unique user ID  |\n",
    "| os | The operating system of the device from which the social network is used  |\n",
    "| source  | Whether the user came to the app from organic traffic or from ads  |\n",
    "\n",
    "**post_text_df:**\n",
    "\n",
    "| Field name | Overview |\n",
    "|---|---|\n",
    "| id  | Unique post ID  |\n",
    "| text  | Text content of the post  |\n",
    "| topic  | Main theme |\n",
    "\n",
    "**feed_data**:\n",
    "\n",
    "| Field name  | Overview  |\n",
    "|---|---|\n",
    "| timestamp  | The time the viewing was made  |\n",
    "| user_id | id of the user who viewed the post |\n",
    "| post_id  | viewed post id  |\n",
    "| action  | Action Type: View or Like  |\n",
    "| target  | Views have 1 if a like was made almost immediately after viewing, otherwise 0. Like actions have a missing value.  |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71227824-7f89-4c35-b087-bfe6f9b82338",
   "metadata": {},
   "source": [
    "### Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c653c3f8-41c1-42ab-940a-180179bc8e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2017c62a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "80beb973-8be1-4e4a-8359-63ca91b26452",
   "metadata": {},
   "source": [
    "### Database connection and used tables overview\n",
    "\n",
    "In this work PostgreSQL was used as a RDBMS. I created `connection` variable for datebase access. When publishing a project on `github`, it will be removed. To demonstrate the work of the web service, a small part of the processed data will be given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d129856-53a6-4fff-9379-a036fbf08fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "connection = \"postgresql://robot-startml-ro:pheiph0hahj1Vaif@postgres.lab.karpov.courses:6432/startml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca419207",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'psycopg2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-23ea63322159>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m### Users data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m user_info = pd.read_sql(\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;34m\"\"\"SELECT * FROM public.user_data\"\"\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/io/sql.py\u001b[0m in \u001b[0;36mread_sql\u001b[0;34m(sql, con, index_col, coerce_float, params, parse_dates, columns, chunksize)\u001b[0m\n\u001b[1;32m    490\u001b[0m     \u001b[0mread_sql_query\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mRead\u001b[0m \u001b[0mSQL\u001b[0m \u001b[0mquery\u001b[0m \u001b[0minto\u001b[0m \u001b[0ma\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m     \"\"\"\n\u001b[0;32m--> 492\u001b[0;31m     \u001b[0mpandas_sql\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpandasSQL_builder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpandas_sql\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSQLiteDatabase\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/io/sql.py\u001b[0m in \u001b[0;36mpandasSQL_builder\u001b[0;34m(con, schema, meta, is_cursor)\u001b[0m\n\u001b[1;32m    664\u001b[0m     \u001b[0;31m# When support for DBAPI connections is removed,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    665\u001b[0m     \u001b[0;31m# is_cursor should not be necessary.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 666\u001b[0;31m     \u001b[0mcon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_engine_builder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    667\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_sqlalchemy_connectable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    668\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mSQLDatabase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmeta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/io/sql.py\u001b[0m in \u001b[0;36m_engine_builder\u001b[0;34m(con)\u001b[0m\n\u001b[1;32m    651\u001b[0m             \u001b[0m_SQLALCHEMY_INSTALLED\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    652\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 653\u001b[0;31m             \u001b[0mcon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msqlalchemy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    654\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mcon\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    655\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<string>\u001b[0m in \u001b[0;36mcreate_engine\u001b[0;34m(url, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/sqlalchemy/util/deprecations.py\u001b[0m in \u001b[0;36mwarned\u001b[0;34m(fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    296\u001b[0m                         \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m                     )\n\u001b[0;32m--> 298\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m         \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/sqlalchemy/engine/create.py\u001b[0m in \u001b[0;36mcreate_engine\u001b[0;34m(url, **kwargs)\u001b[0m\n\u001b[1;32m    546\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m                 \u001b[0mdbapi_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpop_kwarg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 548\u001b[0;31m         \u001b[0mdbapi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdialect_cls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdbapi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mdbapi_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    549\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m     \u001b[0mdialect_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"dbapi\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdbapi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/sqlalchemy/dialects/postgresql/psycopg2.py\u001b[0m in \u001b[0;36mdbapi\u001b[0;34m(cls)\u001b[0m\n\u001b[1;32m    751\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    752\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdbapi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 753\u001b[0;31m         \u001b[0;32mimport\u001b[0m \u001b[0mpsycopg2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    754\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    755\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mpsycopg2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'psycopg2'"
     ]
    }
   ],
   "source": [
    "### Users data\n",
    "\n",
    "user_info = pd.read_sql(\n",
    "    \"\"\"SELECT * FROM public.user_data\"\"\",\n",
    "\n",
    "    con=connection\n",
    ")\n",
    "\n",
    "user_info.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88adbbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install psycopg2-binary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d91cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Посты и топики\n",
    "\n",
    "posts_info = pd.read_sql(\n",
    "    \"\"\"SELECT * FROM public.post_text_df\"\"\",\n",
    "    \n",
    "    con=\"postgresql://robot-startml-ro:pheiph0hahj1Vaif@\"\n",
    "        \"postgres.lab.karpov.courses:6432/startml\"\n",
    ")\n",
    "\n",
    "posts_info.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9354d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Почти 77 миллионов записей, многовато!\n",
    "### Не в каждую оперативу влезет столько данных\n",
    "### Для обучения моделей\n",
    "\n",
    "count_feed_data = pd.read_sql(\n",
    "    \"\"\"SELECT count(*) FROM public.feed_data\"\"\",\n",
    "    \n",
    "    con=\"postgresql://robot-startml-ro:pheiph0hahj1Vaif@\"\n",
    "        \"postgres.lab.karpov.courses:6432/startml\"\n",
    ")\n",
    "\n",
    "count_feed_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1cb383e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Попробуем забрать, скажем, 10 миллионов\n",
    "\n",
    "feed_data = pd.read_sql(\n",
    "    \"\"\"SELECT * FROM public.feed_data LIMIT 10000000\"\"\",\n",
    "    \n",
    "    con=\"postgresql://robot-startml-ro:pheiph0hahj1Vaif@\"\n",
    "        \"postgres.lab.karpov.courses:6432/startml\"\n",
    ")\n",
    "\n",
    "feed_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb15187f",
   "metadata": {},
   "source": [
    "### Работа с данными и фичи для контентной модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815ae2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Как устроена колонка target?\n",
    "\n",
    "feed_data[feed_data.action!='view']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89fab3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Почистим данные от action != view\n",
    "\n",
    "feed_data = feed_data[feed_data.action=='view']\n",
    "\n",
    "feed_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48651afc",
   "metadata": {},
   "source": [
    "Напомним, как устроен контентный подход! \n",
    "\n",
    "Научимся по выбранной дате timestamp \n",
    "\n",
    "И для любой пары (user_id, post_id)\n",
    "\n",
    "Предсказывать, случится лайк или нет\n",
    "\n",
    "Хорошо бы иметь модель, которая умеет предсказывать вероятности"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78f029e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Необходимо по user_id выделять \n",
    "### Фиксированный набор признаков\n",
    "### В целом, подойдет оригинальный датасет\n",
    "\n",
    "user_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdfa9e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "### C post_id куда интереснее!\n",
    "### Придумаем какой-нибудь эмбеддинг \n",
    "### Для текстов\n",
    "\n",
    "posts_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47a9d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def preprocessing(line, token=wnl):\n",
    "    line = line.lower()\n",
    "    line = re.sub(r\"[{}]\".format(string.punctuation), \" \", line)\n",
    "    line = line.replace('\\n\\n', ' ').replace('\\n', ' ')\n",
    "    line = ' '.join([token.lemmatize(x) for x in line.split(' ')])\n",
    "    return line\n",
    "\n",
    "\n",
    "tfidf = TfidfVectorizer(\n",
    "    stop_words='english',\n",
    "    preprocessor=preprocessing\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571727d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_data = (\n",
    "    tfidf\n",
    "    .fit_transform(posts_info['text'])\n",
    "    .toarray()\n",
    ")\n",
    "\n",
    "tfidf_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7d9d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_data = pd.DataFrame(\n",
    "    tfidf_data,\n",
    "    index=posts_info.post_id,\n",
    "    columns=tfidf.get_feature_names_out()\n",
    ")\n",
    "\n",
    "tfidf_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576d7a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Генерим фичи на основе TfIdf\n",
    "\n",
    "posts_info['TotalTfIdf'] = tfidf_data.sum(axis=1).reset_index()[0]\n",
    "posts_info['MaxTfIdf'] = tfidf_data.max(axis=1).reset_index()[0]\n",
    "posts_info['MeanTfIdf'] = tfidf_data.mean(axis=1).reset_index()[0]\n",
    "\n",
    "posts_info.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e43da0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "[f\"DistanceTo{ith}thCluster\" for ith in range(1, 16)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80ec7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Пытаемся кластеризовать тексты\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "centered = tfidf_data - tfidf_data.mean()\n",
    "\n",
    "pca = PCA(n_components=20)\n",
    "pca_decomp = pca.fit_transform(centered)\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans = KMeans(n_clusters=15, random_state=0).fit(pca_decomp)\n",
    "\n",
    "posts_info['TextCluster'] = kmeans.labels_\n",
    "\n",
    "dists_columns = ['DistanceTo1thCluster',\n",
    "                 'DistanceTo2thCluster',\n",
    "                 'DistanceTo3thCluster',\n",
    "                 'DistanceTo4thCluster',\n",
    "                 'DistanceTo5thCluster',\n",
    "                 'DistanceTo6thCluster',\n",
    "                 'DistanceTo7thCluster',\n",
    "                 'DistanceTo8thCluster',\n",
    "                 'DistanceTo9thCluster',\n",
    "                 'DistanceTo10thCluster',\n",
    "                 'DistanceTo11thCluster',\n",
    "                 'DistanceTo12thCluster',\n",
    "                 'DistanceTo13thCluster',\n",
    "                 'DistanceTo14thCluster',\n",
    "                 'DistanceTo15thCluster']\n",
    "\n",
    "dists_df = pd.DataFrame(\n",
    "    data=kmeans.transform(pca_decomp),\n",
    "    columns=dists_columns\n",
    ")\n",
    "\n",
    "dists_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae2ab39",
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_info = pd.concat((posts_info,dists_df), axis=1)\n",
    "\n",
    "posts_info.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf84fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Воспроизведем датафрейм со всеми новыми фичами \n",
    "\n",
    "df = pd.merge(feed_data,\n",
    "              posts_info,\n",
    "              on='post_id',\n",
    "              how='left')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc66ecb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(df,\n",
    "              user_info,\n",
    "              on='user_id',\n",
    "              how='left')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c182d132",
   "metadata": {},
   "outputs": [],
   "source": [
    "### А еще научимся выделять признаки\n",
    "### Из timestamp!\n",
    "### Согласитесь, от времени просмотра может зависеть\n",
    "### Склонность пользователей лайкать или игнорировать посты\n",
    "\n",
    "df['hour'] = pd.to_datetime(df['timestamp']).apply(lambda x: x.hour)\n",
    "df['month'] = pd.to_datetime(df['timestamp']).apply(lambda x: x.month)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a747f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Уберем все ненужные колонки\n",
    "\n",
    "df = df.drop([\n",
    "#    'timestamp',  ### timestamp пока оставим\n",
    "    'action',\n",
    "    'text',\n",
    "],\n",
    "    axis=1)\n",
    "\n",
    "df = df.set_index(['user_id', 'post_id'])\n",
    "\n",
    "df.head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa22771",
   "metadata": {},
   "source": [
    "### Пора обучать модели! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8c19f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Предлагаю начать с относительно простой модели\n",
    "### Например, с решающего дерева\n",
    "### А потом посмотреть уже в сторону бустингов\n",
    "\n",
    "### Как валидировать? Как разобьем на train и test?\n",
    "### Предлагаю по времени, так как данные имеют \n",
    "### Временную структуру! Хотим корректно оценивать\n",
    "### Вероятности для будущих рекомендаций\n",
    "\n",
    "max(df.timestamp), min(df.timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb51ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### За отсечку возьмем 2021-12-15\n",
    "\n",
    "df_train = df[df.timestamp < '2021-12-15']\n",
    "df_test = df[df.timestamp >= '2021-12-15']\n",
    "\n",
    "df_train = df_train.drop('timestamp', axis=1)\n",
    "df_test = df_test.drop('timestamp', axis=1)\n",
    "\n",
    "X_train = df_train.drop('target', axis=1)\n",
    "X_test = df_test.drop('target', axis=1)\n",
    "\n",
    "y_train = df_train['target']\n",
    "y_test = df_test['target']\n",
    "\n",
    "y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e7d3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc339e4f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Начнем с решающего дерева!\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from category_encoders import TargetEncoder\n",
    "from category_encoders.one_hot import OneHotEncoder\n",
    "\n",
    "object_cols = [\n",
    "    'topic', 'TextCluster', 'gender', 'country',\n",
    "    'city', 'exp_group', 'hour', 'month',\n",
    "    'os', 'source'\n",
    "]\n",
    "\n",
    "cols_for_ohe = [x for x in object_cols if X_train[x].nunique() < 5]\n",
    "cols_for_mte = [x for x in object_cols if X_train[x].nunique() >= 5]\n",
    "\n",
    "### Cохраним индексы этих колонок\n",
    "\n",
    "cols_for_ohe_idx = [list(X_train.columns).index(col) for col in cols_for_ohe]\n",
    "cols_for_mte_idx = [list(X_train.columns).index(col) for col in cols_for_mte]\n",
    "\n",
    "t = [\n",
    "    ('OneHotEncoder', OneHotEncoder(), cols_for_ohe_idx),\n",
    "    ('MeanTargetEncoder', TargetEncoder(), cols_for_mte_idx)\n",
    "]\n",
    "\n",
    "col_transform = ColumnTransformer(transformers=t)\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipe_dt = Pipeline([(\"column_transformer\",\n",
    "                     col_transform),\n",
    "                     \n",
    "                    (\"decision_tree\", \n",
    "                     DecisionTreeClassifier())])\n",
    "\n",
    "pipe_dt.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a83f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Замерим качество работы такой модели\n",
    "### Возьмем ROC-AUC\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "print(f\"Качество на трейне: {roc_auc_score(y_train, pipe_dt.predict_proba(X_train)[:, 1])}\")\n",
    "print(f\"Качество на тесте: {roc_auc_score(y_test, pipe_dt.predict_proba(X_test)[:, 1])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cad0dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Теперь обучим катбуст!\n",
    "\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "catboost = CatBoostClassifier(iterations=100,\n",
    "                              learning_rate=1,\n",
    "                              depth=2)\n",
    "\n",
    "catboost.fit(X_train, y_train, object_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7b95e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Замерим качество работы такой модели\n",
    "### Возьмем ROC-AUC\n",
    "\n",
    "print(f\"Качество на трейне: {roc_auc_score(y_train, catboost.predict_proba(X_train)[:, 1])}\")\n",
    "print(f\"Качество на тесте: {roc_auc_score(y_test, catboost.predict_proba(X_test)[:, 1])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e75ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Из любопытства посмотрим на feature_importance\n",
    "\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_feature_importance(importance,names,model_type):\n",
    "    \n",
    "    #Create arrays from feature importance and feature names\n",
    "    feature_importance = np.array(importance)\n",
    "    feature_names = np.array(names)\n",
    "    \n",
    "    #Create a DataFrame using a Dictionary\n",
    "    data={'feature_names':feature_names,'feature_importance':feature_importance}\n",
    "    fi_df = pd.DataFrame(data)\n",
    "    \n",
    "    #Sort the DataFrame in order decreasing feature importance\n",
    "    fi_df.sort_values(by=['feature_importance'], ascending=False,inplace=True)\n",
    "    \n",
    "    #Define size of bar plot\n",
    "    plt.figure(figsize=(10,8))\n",
    "    #Plot Searborn bar chart\n",
    "    sns.barplot(x=fi_df['feature_importance'], y=fi_df['feature_names'])\n",
    "    #Add chart labels\n",
    "    plt.title(model_type + 'FEATURE IMPORTANCE')\n",
    "    plt.xlabel('FEATURE IMPORTANCE')\n",
    "    plt.ylabel('FEATURE NAMES')\n",
    "    \n",
    "plot_feature_importance(catboost.feature_importances_,X_train.columns,'Catboost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19699f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Сохраним модель\n",
    "\n",
    "catboost.save_model(\n",
    "    'catboost_model',\n",
    "    format=\"cbm\"                  \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b4f6e5",
   "metadata": {},
   "source": [
    "### Положим в базу фичи, необходимые для функционала нашей модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf15075",
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_info.to_sql(    \n",
    "   \"posts_info_features\",                    \n",
    "    con=\"postgresql://robot-startml-ro:pheiph0hahj1Vaif@\"\n",
    "        \"postgres.lab.karpov.courses:6432/startml\",                      \n",
    "    schema=\"public\",                   \n",
    "    if_exists='replace'            \n",
    "   )                               \n",
    "                                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e44fa30",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Все ли норм?\n",
    "\n",
    "test_ = pd.read_sql(\n",
    "    \"\"\"SELECT * FROM public.posts_info_features\"\"\",\n",
    "    \n",
    "    con=\"postgresql://robot-startml-ro:pheiph0hahj1Vaif@\"\n",
    "        \"postgres.lab.karpov.courses:6432/startml\"\n",
    ")\n",
    "\n",
    "test_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
